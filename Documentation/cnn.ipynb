{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b8fad8",
   "metadata": {},
   "source": [
    "#   Dokumentacja klasyfikatora CIFAR-10 używając CNN\n",
    "\n",
    "Autor: Filip Gębala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778b25b",
   "metadata": {},
   "source": [
    "<h2> Model CNN </h2>\n",
    "<p>Konwolucyjna sieć neuronowa (CNN) to rodzaj sieci neuronowej typu feedforward, która uczy się istotnych cech danych poprzez optymalizację filtrów (zwanych też jądrami konwolucyjnymi). Tego typu sieci znajdują zastosowanie w różnych dziedzinach — od analizy obrazów, przez przetwarzanie tekstu, aż po dane dźwiękowe. W kontekście analizy obrazów, CNN stały się standardowym rozwiązaniem w zadaniach związanych z widzeniem komputerowym i przetwarzaniem obrazów.</p>\n",
    "\n",
    "<p>Model oparty jest na czterech warstwach konwolucyjnych, z których każda ma na celu wyodrębnienie coraz bardziej złożonych cech obrazu. Początkowo dane wejściowe (obrazy RGB z CIFAR-10) trafiają do warstwy konwolucyjnej z 64 filtrami, a następnie przechodzą przez normalizację batchową i funkcję aktywacji ReLU. Potem następuje operacja max pooling, która zmniejsza rozmiar przestrzenny cech, zachowując przy tym najważniejsze informacje. Ten schemat jest powtarzany w kolejnych warstwach, z rosnącą liczbą filtrów: 128, 256 i 512.\n",
    "\n",
    "Po ostatniej warstwie konwolucyjnej zastosowana została warstwa global average pooling, która redukuje każdą mapę cech do jednej wartości, co znacząco zmniejsza liczbę parametrów przed przejściem do warstwy w pełni połączonej. Przed klasyfikatorem dodano jeszcze dropout (z prawdopodobieństwem 0.5), co pomaga zapobiegać przeuczeniu. Ostateczna klasyfikacja odbywa się przez pojedynczą warstwę liniową, która zwraca 10 wartości odpowiadających klasom CIFAR-10. Cały model trenowany jest przy użyciu optymalizatora Adam z regularyzacją wag (weight decay) i harmonogramem uczenia, który stopniowo zmniejsza learning rate, by poprawić stabilność treningu.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84819e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0091",
   "metadata": {},
   "source": [
    "<h2>Przygotowanie danych </h2>\n",
    "\n",
    "<p>Przed rozpoczęciem treningu dane muszą zostać odpowiednio przetworzone. Dla zbioru treningowego zastosowano augmentację danych: obrazy są losowo kadrowane z dodanym paddingiem oraz mogą zostać odbite w poziomie. To sprawia, że sieć widzi więcej różnorodnych wariantów obrazków i lepiej generalizuje. Następnie dane są zamieniane na tensory i znormalizowane — czyli przesunięte i przeskalowane tak, by każda składowa kolorystyczna (R, G, B) miała średnią i odchylenie standardowe odpowiadające wartościom w zbiorze CIFAR-10. Dla danych testowych nie stosuje się augmentacji, tylko sama normalizacja.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRANSFORMACJE ===\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                        (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                        (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2dc5d",
   "metadata": {},
   "source": [
    "<h2>Ładowanie danych oraz inicjacja modelu</h2>\n",
    "\n",
    "<p>Po wczytaniu danych do loaderów, inicjalizowany jest model oraz optymalizator Adam. Zastosowano też regularyzację L2 (czyli weight decay), by ograniczyć nadmierne dopasowanie modelu do danych treningowych. Do tego dorzucono scheduler, który co 10 epok zmniejsza learning rate o połowę — taki zabieg pomaga przy konwergencji, zwłaszcza w późniejszych etapach uczenia. Funkcja straty to klasyczna entropia krzyżowa, odpowiednia przy klasyfikacji wieloklasowej.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DANE ===\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# === MODEL I OPT ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff970d",
   "metadata": {},
   "source": [
    "<h2>Trenowanie modelu </h2>\n",
    "\n",
    "<p>Sam proces uczenia podzielony jest na epoki. W każdej z nich model przechodzi przez wszystkie próbki w zbiorze treningowym. Dla każdej partii obliczane są predykcje, wyznaczana strata, a potem wykonywany jest krok optymalizacji. Po zakończeniu jednej epoki, model oceniany jest na zbiorze testowym. Wynik zapisywany jest w listach train_losses i test_accuracies, żeby można było później narysować wykresy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d317cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRENING ===\n",
    "def train_model(model, trainloader, testloader, optimizer, scheduler, criterion, device, epochs):\n",
    "    train_losses, test_accuracies = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(trainloader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Ewaluacja\n",
    "        accuracy = test_model(model, testloader, device)\n",
    "        test_accuracies.append(accuracy)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc33ba",
   "metadata": {},
   "source": [
    "<h2>Ewaluacja modelu </h2>\n",
    "\n",
    "<p>Do ewaluacji testowej używana jest oddzielna funkcja test_model, która przełącza model w tryb \"eval\" — wyłącza on liczenie gradientów i stosowanie dropoutu. Dla każdej partii danych z testu sprawdzana jest poprawność klasyfikacji, a na końcu wyliczana jest całkowita dokładność.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777443cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "def test_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436220c",
   "metadata": {},
   "source": [
    "<h2>Wykresy </h2>\n",
    "\n",
    "<p>Na końcu wykreślane są wykresy — pierwszy pokazuje, jak zmieniała się wartość funkcji straty podczas treningu, drugi przedstawia zmiany dokładności na zbiorze testowym. Dzięki temu łatwo stwierdzić, w którym miejscu model zaczynał się przeuczać</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WYKRESY ===\n",
    "epochs = range(1, len(train_losses)+1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.title('Strata treningowa')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy', color='orange')\n",
    "plt.title('Dokładność na zbiorze testowym')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
