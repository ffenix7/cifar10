{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b8fad8",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classifier Documentation using CNN\n",
    "\n",
    "Author: Filip Gębala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778b25b",
   "metadata": {},
   "source": [
    "<h2>Model CNN</h2>\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a type of feedforward neural network that learns meaningful data features by optimizing filters (also called convolutional kernels). CNNs are widely used in various domains—from image analysis, through text processing, to audio data. In image analysis, CNNs have become the standard solution for computer vision and image processing tasks.\n",
    "\n",
    "The model is based on four convolutional layers, each designed to extract increasingly complex image features. Input data (RGB images from CIFAR-10) first pass through a convolutional layer with 64 filters, followed by batch normalization and a ReLU activation. A max pooling operation then reduces dimensions while preserving key information. This pattern repeats in subsequent layers with 128, 256, and 512 filters.\n",
    "\n",
    "After the last convolutional layer, a global average pooling layer reduces each feature map to a single value, significantly decreasing the number of parameters before the fully connected layer. Before the classifier, a dropout layer with probability 0.5 helps prevent overfitting. Final classification is performed by a single linear layer that outputs 10 values corresponding to the CIFAR-10 classes. The entire model is trained using the Adam optimizer with weight decay and a learning rate scheduler that decreases the rate over time to improve training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84819e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL ===\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0091",
   "metadata": {},
   "source": [
    "<h2>Data Preparation</h2>\n",
    "\n",
    "Before training, data must be appropriately processed. For the training set, data augmentation includes random cropping with padding and random horizontal flips. This exposes the network to diverse image variants and improves generalization. Afterwards, images are converted to tensors and normalized so that each color channel (R, G, B) has the mean and standard deviation of the CIFAR-10 dataset. For the test set, only normalization is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRANSFORMATIONS ===\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.RandomHorizontalFlip(),     \n",
    "    transforms.ToTensor(),                 \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),  \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2dc5d",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Model Initialization</h2>\n",
    "\n",
    "After loading data into DataLoaders, the model and the Adam optimizer are initialized. L2 regularization (weight decay) is applied to limit overfitting. A scheduler reduces the learning rate by half every 10 epochs to help convergence in later training stages. The loss function is CrossEntropyLoss, suitable for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADING ===\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# === MODEL & OPTIMIZER ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff970d",
   "metadata": {},
   "source": [
    "<h2>Model training</h2>\n",
    "\n",
    "The training process is divided into epochs. In each epoch, the model iterates over all batches in the training set. For each batch, predictions are made, the loss is computed, and an optimization step is performed. At the end of each epoch, the model is evaluated on the test set. Training losses, test accuracies, and confusion matrices are recorded for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d317cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING ===\n",
    "\n",
    "def train_model(model, trainloader, testloader, optimizer, scheduler, criterion, device, epochs):\n",
    "    train_losses, test_accuracies, confusion_matrices = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(trainloader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Evaluate model and get confusion matrix\n",
    "        accuracy, cm = test_model(model, testloader, device, return_confusion=True)\n",
    "        test_accuracies.append(accuracy)\n",
    "        confusion_matrices.append(cm)  # Store confusion matrix\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, test_accuracies, confusion_matrices\n",
    "\n",
    "# === EXECUTION ===\n",
    "train_losses, test_accuracies, confusion_matrices = train_model(\n",
    "    model, trainloader, testloader, optimizer, scheduler, criterion, device, epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc33ba",
   "metadata": {},
   "source": [
    "<h2>Model testing </h2>\n",
    "\n",
    "The test_model function switches the model to evaluation mode—disabling gradient calculation and dropout. It computes predictions for the test set, calculates accuracy, and optionally returns the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777443cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "def test_model(model, testloader, device, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    if return_confusion:\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return accuracy, cm\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436220c",
   "metadata": {},
   "source": [
    "<h2> Plots </h2>\n",
    "\n",
    "At the end, three plots are generated: one showing the training loss over epochs, second one showing the test accuracy and the third one being a confussion matrix. This helps to evaluate models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b39b2bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# === PLOTS ===\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epochs_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_losses\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# === PLOTS ===\n",
    "epochs_range = range(1, len(train_losses)+1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, test_accuracies, label='Test Accuracy', color='orange')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrices[-1], display_labels=testset.classes)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Final Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
