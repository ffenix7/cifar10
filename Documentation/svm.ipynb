{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54415c27",
   "metadata": {},
   "source": [
    "#   Dokumentacja klasyfikatora CIFAR-10 używając SVM\n",
    "\n",
    "Autor: Filip Gębala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77294d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3521bd",
   "metadata": {},
   "source": [
    "<h2>Model SVM</h2> <p>Maszyna wektorów nośnych (SVM, ang. Support Vector Machine) to nadzorowana metoda klasyfikacji, która znajduje optymalną hiperpłaszczyznę separującą dane w przestrzeni cech. Model ten stara się zmaksymalizować margines — odległość między hiperpłaszczyzną a najbliższymi punktami danych obu klas — co prowadzi do lepszej ogólności modelu i odporności na szum.</p> <p>W zastosowaniu do danych obrazowych z CIFAR-10, obrazy RGB są najpierw przekształcane do formatu tensora, a następnie spłaszczane do postaci wektorów cech. Ze względu na dużą wymiarowość danych (3072 cechy na obraz), przed klasyfikacją stosowana jest standaryzacja oraz redukcja wymiarowości metodą analizy głównych składowych (PCA). PCA redukuje dane do 300 wymiarów, co nie tylko przyspiesza trenowanie, ale także pomaga usunąć szum i zależności między cechami.</p> <p>Sam klasyfikator SVM wykorzystuje radialną funkcję bazową (RBF) jako jądro, co pozwala mu modelować nieliniowe granice decyzyjne. Model został wytrenowany z parametrem regularyzacji <code>C=10</code> oraz automatycznym dostrojeniem <code>gamma</code> do wartości „scale”, co zapewnia dobre dopasowanie przy umiarkowanym ryzyku przeuczenia. Trenowanie przeprowadzono na podzbiorze 100 000 próbek treningowych, a testowanie — na 1 000 próbkach z zestawu testowego CIFAR-10.</p> <p>Dzięki uprzedniej redukcji wymiarów oraz standaryzacji, klasyfikator osiąga przyzwoitą dokładność, biorąc pod uwagę ograniczenia związane z brakiem specjalistycznej ekstrakcji cech (jak w CNN). Model SVM dobrze sprawdza się jako klasyfikator bazowy lub komponent większych systemów klasyfikacyjnych, zwłaszcza w połączeniu z technikami redukcji wymiarów i przetwarzania wstępnego.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e1eb6",
   "metadata": {},
   "source": [
    "<h2>Przygotowanie danych</h2>\n",
    "<p>Podobnie jak w przypadku CNN, dane są ładowane przy użyciu `torchvision`. Jednak w tym przypadku nie stosuje się żadnej augmentacji ani normalizacji, ponieważ dane będą później przekształcane z użyciem `StandardScaler`. Dodatkowo, ponieważ pełny zbiór CIFAR-10 zawiera 50 tysięcy próbek, do treningu używamy podzbioru danych (np. 10 000), żeby skrócić czas treningu SVM. Każdy obraz zostaje spłaszczony do wektora, aby można było go użyć jako wejście do SVM.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37286c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRANSFORMACJA ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                    (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# === ZAŁADUJ ZBIÓR (trenuj na próbce) ===\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Próbkuj mały podzbiór — pełen CIFAR-10 zajmuje za dużo czasu\n",
    "def get_data(dataset, n_samples):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=n_samples, shuffle=True)\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.view(images.size(0), -1)  # flatten\n",
    "    return images.numpy(), labels.numpy()\n",
    "\n",
    "X_train, y_train = get_data(trainset, 100000)\n",
    "X_test, y_test = get_data(testset, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73544700",
   "metadata": {},
   "source": [
    "<h2>Preprocessing: Standaryzacja i PCA</h2>\n",
    "<p>Standaryzacja danych jest kluczowa przy stosowaniu SVM. Następnie przeprowadzana jest redukcja wymiarowości za pomocą PCA, co pozwala przyspieszyć uczenie i poprawia stabilność algorytmu. Zmniejszamy ilość wymiarów z 3072 do 300.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1143f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# === PCA ===\n",
    "pca = PCA(n_components=300)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575abed7",
   "metadata": {},
   "source": [
    "<h2>Trenowanie modelu SVM</h2>\n",
    "<p>SVM jest trenowane przy użyciu jądra RBF (gaussowskiego). Parametr `C=10` ustala stopień karania błędów klasyfikacji, a `gamma='scale'` automatycznie dobiera szerokość jądra RBF do danych. Trening może trwać kilka minut w zależności od liczby próbek i wydajności komputera.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRENING ===\n",
    "clf = svm.SVC(kernel='rbf', C=10, gamma='scale')\n",
    "print(\"Trenowanie\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# === TEST ===\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Dokładność na zbiorze testowym: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc04f2a",
   "metadata": {},
   "source": [
    "<h2>Wyniki i podsumowanie </h2>\n",
    "<p>SVM może osiągać całkiem dobre wyniki nawet bez głębokiego uczenia, szczególnie przy odpowiedniej redukcji wymiarowości (PCA) i dobrze dobranych parametrach. W porównaniu do CNN, SVM jest mniej podatne na przeuczenie, ale jego skuteczność może być ograniczona przez brak nieliniowych, wielowarstwowych reprezentacji danych.</p>\n",
    "\n",
    "<p>W tym przypadku niestety SVM nie był dobrym wyborem - uzyskał dokładność na poziomie ~55%. Jest to spowodowane głównie dużym rozmiarem obrazów, co bardzo zniekształca hiperpłaszczyznę.</p>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
