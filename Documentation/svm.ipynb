{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54415c27",
   "metadata": {},
   "source": [
    "#   CIFAR-10 Classifier Documentation Using SVM\n",
    "\n",
    "Author: Filip Gębala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77294d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3521bd",
   "metadata": {},
   "source": [
    "## SVM Model\n",
    "Support Vector Machine (SVM) is a supervised classification method that finds the optimal hyperplane separating data in feature space. The model aims to maximize the margin — the distance between the hyperplane and the nearest data points from both classes — leading to better generalization and resistance to noise.\n",
    "\n",
    "When applied to CIFAR-10 image data, RGB images are first transformed into tensors and then flattened into feature vectors. Due to the high dimensionality of the data (3072 features per image), before classification, standardization and dimensionality reduction using Principal Component Analysis (PCA) are applied. PCA reduces the data to 300 dimensions, which not only speeds up training but also helps remove noise and correlations between features.\n",
    "\n",
    "The SVM classifier uses the Radial Basis Function (RBF) kernel, which allows it to model non-linear decision boundaries. The model is trained with the regularization parameter C=10 and an automatically tuned gamma set to \"scale\", ensuring a good fit with moderate overfitting risk. Training is done on a subset of 100,000 training samples, and testing is performed on 1,000 samples from the CIFAR-10 test set.\n",
    "\n",
    "Thanks to dimensionality reduction and standardization, the classifier achieves decent accuracy considering the limitations of not using specialized feature extraction (like in CNNs). The SVM model works well as a baseline classifier or a component of larger classification systems, especially when combined with dimensionality reduction and preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e1eb6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Similar to CNNs, data is loaded using torchvision. However, no augmentation or normalization is applied because the data will later be transformed using StandardScaler. Additionally, since the full CIFAR-10 dataset contains 50,000 samples, we use a subset of the data (e.g., 10,000) to shorten the training time for SVM. Each image is flattened into a vector for use as input to the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37286c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRANSFORMATION ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# === LOAD DATASET (train on a sample) ===\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Sample a small subset\n",
    "def get_data(dataset, n_samples):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=n_samples, shuffle=True)\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.view(images.size(0), -1)\n",
    "    return images.numpy(), labels.numpy()\n",
    "\n",
    "X_train, y_train = get_data(trainset, 100000)\n",
    "X_test, y_test = get_data(testset, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73544700",
   "metadata": {},
   "source": [
    "## Preprocessing: Standardization and PCA\n",
    "Standardizing the data is crucial when using SVM. After that, dimensionality reduction is performed using PCA, which speeds up training and improves algorithm stability. The dimensionality is reduced from 3072 to 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1143f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# === PCA ===\n",
    "pca = PCA(n_components=300)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575abed7",
   "metadata": {},
   "source": [
    "## Training the SVM Model\n",
    "The SVM is trained using the RBF (Gaussian) kernel. The C=10 parameter sets the penalty for classification errors, and gamma='scale' automatically adjusts the RBF kernel width to the data. Training may take a few minutes depending on the number of samples and the performance of the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING ===\n",
    "clf = svm.SVC(kernel='rbf', C=10, gamma='scale')\n",
    "print(\"Training...\")\n",
    "clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ee298",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TESTING ===\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f46085",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "1. Explained Variance by PCA\n",
    "<br>\n",
    "To evaluate how much of the variance is explained by the reduced components, we can visualize the cumulative explained variance using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.title(\"Cumulative Explained Variance by PCA\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250d3ea",
   "metadata": {},
   "source": [
    "2. PCA for Visualization in 2D\n",
    "<br>\n",
    "We can visualize the data in two dimensions to get an intuitive understanding of how well PCA has separated the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "X_vis = pca_2d.fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_train, cmap='tab10', alpha=0.5, s=10)\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.title(\"Training Data after PCA (2 Components)\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9bfe6",
   "metadata": {},
   "source": [
    "3. Confusion Matrix \n",
    "<br>\n",
    "To further analyze the performance of the model, the confusion matrix can provide detailed insights into misclassifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92629e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=trainset.classes, yticklabels=trainset.classes)\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - SVM on CIFAR-10\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc04f2a",
   "metadata": {},
   "source": [
    "## Results and Summary\n",
    "SVM can achieve good results even without deep learning, especially with proper dimensionality reduction (PCA) and well-tuned parameters. Compared to CNNs, SVM is less prone to overfitting, but its effectiveness might be limited by the lack of non-linear, multi-layered data representations.\n",
    "\n",
    "In this case, unfortunately, SVM did not perform well — achieving an accuracy of ~55%. This is mainly due to the large image size, which distorts the hyperplane significantly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
